{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Citibike Data in NYC\n",
    "## Introduction\n",
    "\n",
    "The objective of this project is to analyze a large random sample of Citibike data from the years 2013 to 2019. We will look at different trends between location and month, as well as trends in the type of users and so much more. Hopefully after reading this tutorial, you will not only be able to understand more about different statistics/trends about the Citibike data, but also how much Citibike has become integrated in the lives of New Yorkers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python Modules\n",
    "The main python modules we will be using in our project are:\n",
    "1. folium\n",
    "2. requests\n",
    "3. pandas\n",
    "4. numpy\n",
    "5. re\n",
    "6. datetime\n",
    "7. BeautifulSoup\n",
    "8. json\n",
    "9. matplotlib\n",
    "10. sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium\n",
    "import requests #for get request\n",
    "import pandas as pd #pandas\n",
    "import numpy as np #module\n",
    "import re #regex\n",
    "from datetime import datetime #datetime objects\n",
    "from bs4 import BeautifulSoup #prettify's our content\n",
    "import json #needed for google API\n",
    "import os.path #needed for file reading\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "from sklearn import linear_model #for linear regression\n",
    "from sklearn.preprocessing import PolynomialFeatures #polynomial regression\n",
    "import folium\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "This is the data collection phase of the data life cycle. During this phase, our focus is on collecting and transforming the data into a usable form. In our case, that usable form is a pandas dataframe derived from a CSV file containing the original data. <br> <br>\n",
    "\n",
    "After doing a lot of research for a dataset, we started to look towards the travel industry for motivation. We were motivated by data that contained information regarding ride sharing. At the end, we finalized our search on publicly available bike sharing data from Citi Bike NYC. We retrieved the data from the following site: https://s3.amazonaws.com/tripdata/index.html?fbclid=IwAR3BJ9yWYcKtBRYUMQq2SI1IZR9AnFel3C-eTato4mWXtwBz4olhrdrai7Js\n",
    "\n",
    "The company had available several months worth of data, ranging from June of 2013 to October of 2019, so we decided to explore each of these months. In attempting to utilize the data from the entire data range mentioned, we needed to perform several steps to automate the download and collection process. The following process will describe how we went about collecting and manipulating the data into one aggregate dataframe containing a certain number of entries of each date. <br>\n",
    "\n",
    "To collect and store the data, we used the following libraries from above:\n",
    "1. urllib\n",
    "2. zipfile\n",
    "3. pandas\n",
    "\n",
    "As mentioned before, the data was organized by year, and then month, so it was quite predictable in terms of figuring out how to extract the data files from the site. However, there were some unique aspects that required some workarounds. We looped through each year and month pair from June of 2013 to October of 2019. All of the month numbers in the file names had 2 digits, so we need to make sure all months before October (10th month) were **prepended with a \"0\"**. Most of the zip files followed the format **\"{date}{month_2_digit}-citibike-tripdata.zip\"**. The remaining zip files had a slightly different name format of **\"{date}{month_2_digit}-citibike-tripdata.csv.zip\"**. To get work around this, we instituted a check in the loop to determine if the current year was after 2016. If so, then the url and zip file name would be updated accordingly. \n",
    "\n",
    "Once those alterations were completed, as necessary, we proceeded to download the file data. To do so, we utilized the library function **urllib.request.urlretrieve** to download the zip file, indicated by the corresponding url, to the corresponding file path location, which was the current directory in this case. With the zip file downloaded, we proceeded to extract the contents of the zip file, only one CSV file in this case, using zipfile **extractall** method and extracted it to the current directory. We also retrieved the name of the file using the zipfile **namelist** and indexing at 0, as there is always only one element (CSV file) in said list. <br>\n",
    "    \n",
    "With the CSV data file download and extracted from the corresponding zip file, we transferred it into a pandas dataframe object. We made sure to add year and moth column to differentiate these data points from entries in future CSV files. After that, we decided to add only a sample of the resulting dataframe to our aggregate dataframe. This was due to the fact there were too many entries in every CSV file, so it would be impractical and inefficient to collect every data from each CSV for exploratory data analysis. With that in mind, we decided to randomly sample 10000 rows from each dataframe to be used for data analysis using the pandas **sample** method without replacement. In sampling with n=10000, we get a good representation of the data for that specific date.<br>\n",
    "    \n",
    "One issue we ran into when generating each sampled dataframe was with the column (attribute) names. We realized that, after approximately 2017, the attribute names were capitalized. Due to this difference, the resulting aggregate dataframe at first contained many NaNs because pandas concatenates dataframes based on their column names. The dataframe resulted in having 2 times the normal amount of column names: half being lowercase and half being capitalized. Ignoring case, the column names across all CSV files were identical. To work around this, we decided to create a list of default, lowercase column names, **col_names**, and replaced the column names of each sampled dataframe with our list. <br>\n",
    "    \n",
    "After iterating over each date, we ended up with a **list of 77 dataframes**, each corresponding to a specific date from the data. Since, we made sure that all of these dataframes had the same column names, we proceeded to concatenate all of them into one aggregate dataframe, as mentioned before. With 77 dataframes, each with 10000 randomly sampled data points, the new aggregate dataframe contained **770001** total data points. To do this, we utilized the pandas **concat** function, because it we could pass an iterable, which, in this case, was our list of sampled dataframes. The resulting dataframe was returned as output for display purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The data begins starting on June of 2013. \n",
    "year = 2013\n",
    "month = 6\n",
    "\n",
    "# In some of the csv files, we found that the column names differed by \n",
    "# capitalization, so below is a standard list of the all of the column\n",
    "# names that we apply to each sampled dataframe. \n",
    "col_names = ['tripduration', 'starttime', 'stoptime', 'start station id',\n",
    " 'start station name', 'start station latitude', 'start station longitude',\n",
    " 'end station id', 'end station name', 'end station latitude',\n",
    " 'end station longitude', 'bikeid', 'usertype', 'birth year', 'gender', 'year',\n",
    " 'month']\n",
    "\n",
    "# A list to hold all of the dataframes\n",
    "list_dfs = []\n",
    "\n",
    "# The last csv file contains the data recorded on October of 2019. The\n",
    "# loop will increment by 1 month, until after this date is reached. \n",
    "while (not (year == 2019 and month == 11)):\n",
    "    date_str = f'{year}{month}'\n",
    "    \n",
    "    # If the month is before October (10), prepend the corresponding \n",
    "    # integer value with a 0 to make it 2 digits\n",
    "    if month < 10:\n",
    "        date_str = f'{year}0{month}'\n",
    "    \n",
    "    # Corresponding download URL for each relevant zip file\n",
    "    url = f'https://s3.amazonaws.com/tripdata/{date_str}-citibike-tripdata.zip'\n",
    "    # Corresponding name for each relevant zip file\n",
    "    zip_file = f'{date_str}-citibike-tripdata.zip'\n",
    "    \n",
    "    # Format of zip file name changes after 2016, so this if stmt accounts for that\n",
    "    if (year > 2016):\n",
    "        url = f'https://s3.amazonaws.com/tripdata/{date_str}-citibike-tripdata.csv.zip'\n",
    "        zip_file = f'{date_str}-citibike-tripdata.csv.zip'\n",
    "    \n",
    "    # Zip file path points to the current directory \".\"\n",
    "    zip_file_path = f'./{zip_file}'\n",
    "    \n",
    "    print(year, month)\n",
    "    \n",
    "    # Download the corresponding zip file\n",
    "    urllib.request.urlretrieve(url, zip_file)\n",
    "    \n",
    "    csv_filename = ''\n",
    "    \n",
    "    # Extract the csv from the zip file and get its name\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "        csv_filename = zip_ref.namelist()[0]\n",
    "    \n",
    "    # Read the csv file into a pandas dataframe\n",
    "    df = pd.read_csv(f'./{csv_filename}')\n",
    "    \n",
    "    # Add the year and month attributes to distinguish from entries\n",
    "    # from entries from other dates\n",
    "    df['year'] = year\n",
    "    df['month'] = month  \n",
    "    \n",
    "    # Take a random sample of 10000 data points from the csv\n",
    "    sampled_df = df.sample(10000, replace=False, random_state=1)  \n",
    "    \n",
    "    # Make sure that all of the dataframes have the same attribute formats\n",
    "    sampled_df.columns = list(col_names)    \n",
    "    print(sampled_df.columns.values)\n",
    "    \n",
    "    # Add the dataframe to a list containing all dataframes \n",
    "    list_dfs.append(sampled_df)   \n",
    "    \n",
    "    # Increment by 1 month. Increment the year by 1 if month goes past Dec.\n",
    "    month += 1  \n",
    "    if (month > 12):\n",
    "        month = 1\n",
    "        year += 1\n",
    "\n",
    "# Concatenate all dataframes together to form a cumulative dataframe \n",
    "bikes_df = pd.concat(list_dfs)     \n",
    "bikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dataframe containing sampled observations from every date in the range of June of 2013 to October of 2019, we decided to export it to a CSV file for future use by using the pandas **to_csv** method. The reason behind this was because the process of downloading, extracting, and converting the data from the site was time and memory intensive. Another related thing to note was that kernel did die on occasion and had to restart. For this reason, rather than repeatedly performing these operations every time, we exported to the resulting dataframe to a CSV for efficiency purposes. It will later be converted back to a dataframe object when we begin our data analysis below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a csv file for later use and safe keeping\n",
    "bikes_df.to_csv('./FINAL_SAMPLED_DATA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tripduration</th>\n",
       "      <th>starttime</th>\n",
       "      <th>stoptime</th>\n",
       "      <th>start station id</th>\n",
       "      <th>start station name</th>\n",
       "      <th>start station latitude</th>\n",
       "      <th>start station longitude</th>\n",
       "      <th>end station id</th>\n",
       "      <th>end station name</th>\n",
       "      <th>end station latitude</th>\n",
       "      <th>end station longitude</th>\n",
       "      <th>bikeid</th>\n",
       "      <th>usertype</th>\n",
       "      <th>birth year</th>\n",
       "      <th>gender</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>2013-06-30 19:21:20</td>\n",
       "      <td>2013-06-30 19:26:20</td>\n",
       "      <td>530.0</td>\n",
       "      <td>11 Ave &amp; W 59 St</td>\n",
       "      <td>40.771522</td>\n",
       "      <td>-73.990541</td>\n",
       "      <td>449.0</td>\n",
       "      <td>W 52 St &amp; 9 Ave</td>\n",
       "      <td>40.764618</td>\n",
       "      <td>-73.987895</td>\n",
       "      <td>17034</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>616</td>\n",
       "      <td>2013-06-29 15:40:45</td>\n",
       "      <td>2013-06-29 15:51:01</td>\n",
       "      <td>238.0</td>\n",
       "      <td>Bank St &amp; Washington St</td>\n",
       "      <td>40.736197</td>\n",
       "      <td>-74.008592</td>\n",
       "      <td>79.0</td>\n",
       "      <td>Franklin St &amp; W Broadway</td>\n",
       "      <td>40.719116</td>\n",
       "      <td>-74.006667</td>\n",
       "      <td>19847</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>781</td>\n",
       "      <td>2013-06-26 12:59:35</td>\n",
       "      <td>2013-06-26 13:12:36</td>\n",
       "      <td>435.0</td>\n",
       "      <td>W 21 St &amp; 6 Ave</td>\n",
       "      <td>40.741740</td>\n",
       "      <td>-73.994156</td>\n",
       "      <td>487.0</td>\n",
       "      <td>E 20 St &amp; FDR Drive</td>\n",
       "      <td>40.733143</td>\n",
       "      <td>-73.975739</td>\n",
       "      <td>19195</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1633</td>\n",
       "      <td>2013-06-21 18:42:01</td>\n",
       "      <td>2013-06-21 19:09:14</td>\n",
       "      <td>491.0</td>\n",
       "      <td>E 24 St &amp; Park Ave S</td>\n",
       "      <td>40.740964</td>\n",
       "      <td>-73.986022</td>\n",
       "      <td>243.0</td>\n",
       "      <td>Fulton St &amp; Rockwell Pl</td>\n",
       "      <td>40.688226</td>\n",
       "      <td>-73.979382</td>\n",
       "      <td>20195</td>\n",
       "      <td>Customer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>574</td>\n",
       "      <td>2013-06-02 09:10:43</td>\n",
       "      <td>2013-06-02 09:20:17</td>\n",
       "      <td>398.0</td>\n",
       "      <td>Atlantic Ave &amp; Furman St</td>\n",
       "      <td>40.691652</td>\n",
       "      <td>-73.999979</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Front St &amp; Washington St</td>\n",
       "      <td>40.702551</td>\n",
       "      <td>-73.989402</td>\n",
       "      <td>19606</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769995</td>\n",
       "      <td>477</td>\n",
       "      <td>2019-10-18 18:11:08.4780</td>\n",
       "      <td>2019-10-18 18:19:06.1790</td>\n",
       "      <td>402.0</td>\n",
       "      <td>Broadway &amp; E 22 St</td>\n",
       "      <td>40.740343</td>\n",
       "      <td>-73.989551</td>\n",
       "      <td>267.0</td>\n",
       "      <td>Broadway &amp; W 36 St</td>\n",
       "      <td>40.750977</td>\n",
       "      <td>-73.987654</td>\n",
       "      <td>38486</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1978</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769996</td>\n",
       "      <td>770</td>\n",
       "      <td>2019-10-01 18:52:59.1080</td>\n",
       "      <td>2019-10-01 19:05:50.0560</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>Kent Ave &amp; N 7 St</td>\n",
       "      <td>40.720368</td>\n",
       "      <td>-73.961651</td>\n",
       "      <td>3452.0</td>\n",
       "      <td>Bayard St &amp; Leonard St</td>\n",
       "      <td>40.719156</td>\n",
       "      <td>-73.948854</td>\n",
       "      <td>35125</td>\n",
       "      <td>Customer</td>\n",
       "      <td>1969</td>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769997</td>\n",
       "      <td>250</td>\n",
       "      <td>2019-10-04 15:38:44.3080</td>\n",
       "      <td>2019-10-04 15:42:54.4630</td>\n",
       "      <td>301.0</td>\n",
       "      <td>E 2 St &amp; Avenue B</td>\n",
       "      <td>40.722174</td>\n",
       "      <td>-73.983688</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Allen St &amp; Rivington St</td>\n",
       "      <td>40.720196</td>\n",
       "      <td>-73.989978</td>\n",
       "      <td>34112</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769998</td>\n",
       "      <td>594</td>\n",
       "      <td>2019-10-18 21:08:55.7210</td>\n",
       "      <td>2019-10-18 21:18:49.8250</td>\n",
       "      <td>3707.0</td>\n",
       "      <td>Lexington Ave &amp; E 26 St</td>\n",
       "      <td>40.741459</td>\n",
       "      <td>-73.983293</td>\n",
       "      <td>432.0</td>\n",
       "      <td>E 7 St &amp; Avenue A</td>\n",
       "      <td>40.726218</td>\n",
       "      <td>-73.983799</td>\n",
       "      <td>14545</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1991</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>769999</td>\n",
       "      <td>358</td>\n",
       "      <td>2019-10-19 11:54:33.6470</td>\n",
       "      <td>2019-10-19 12:00:32.1480</td>\n",
       "      <td>497.0</td>\n",
       "      <td>E 17 St &amp; Broadway</td>\n",
       "      <td>40.737050</td>\n",
       "      <td>-73.990093</td>\n",
       "      <td>504.0</td>\n",
       "      <td>1 Ave &amp; E 16 St</td>\n",
       "      <td>40.732219</td>\n",
       "      <td>-73.981656</td>\n",
       "      <td>34991</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>1955</td>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>770000 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        tripduration                 starttime                  stoptime  \\\n",
       "0                300       2013-06-30 19:21:20       2013-06-30 19:26:20   \n",
       "1                616       2013-06-29 15:40:45       2013-06-29 15:51:01   \n",
       "2                781       2013-06-26 12:59:35       2013-06-26 13:12:36   \n",
       "3               1633       2013-06-21 18:42:01       2013-06-21 19:09:14   \n",
       "4                574       2013-06-02 09:10:43       2013-06-02 09:20:17   \n",
       "...              ...                       ...                       ...   \n",
       "769995           477  2019-10-18 18:11:08.4780  2019-10-18 18:19:06.1790   \n",
       "769996           770  2019-10-01 18:52:59.1080  2019-10-01 19:05:50.0560   \n",
       "769997           250  2019-10-04 15:38:44.3080  2019-10-04 15:42:54.4630   \n",
       "769998           594  2019-10-18 21:08:55.7210  2019-10-18 21:18:49.8250   \n",
       "769999           358  2019-10-19 11:54:33.6470  2019-10-19 12:00:32.1480   \n",
       "\n",
       "        start station id        start station name  start station latitude  \\\n",
       "0                  530.0          11 Ave & W 59 St               40.771522   \n",
       "1                  238.0   Bank St & Washington St               40.736197   \n",
       "2                  435.0           W 21 St & 6 Ave               40.741740   \n",
       "3                  491.0      E 24 St & Park Ave S               40.740964   \n",
       "4                  398.0  Atlantic Ave & Furman St               40.691652   \n",
       "...                  ...                       ...                     ...   \n",
       "769995             402.0        Broadway & E 22 St               40.740343   \n",
       "769996            3016.0         Kent Ave & N 7 St               40.720368   \n",
       "769997             301.0         E 2 St & Avenue B               40.722174   \n",
       "769998            3707.0   Lexington Ave & E 26 St               40.741459   \n",
       "769999             497.0        E 17 St & Broadway               40.737050   \n",
       "\n",
       "        start station longitude  end station id          end station name  \\\n",
       "0                    -73.990541           449.0           W 52 St & 9 Ave   \n",
       "1                    -74.008592            79.0  Franklin St & W Broadway   \n",
       "2                    -73.994156           487.0       E 20 St & FDR Drive   \n",
       "3                    -73.986022           243.0   Fulton St & Rockwell Pl   \n",
       "4                    -73.999979          2000.0  Front St & Washington St   \n",
       "...                         ...             ...                       ...   \n",
       "769995               -73.989551           267.0        Broadway & W 36 St   \n",
       "769996               -73.961651          3452.0    Bayard St & Leonard St   \n",
       "769997               -73.983688           401.0   Allen St & Rivington St   \n",
       "769998               -73.983293           432.0         E 7 St & Avenue A   \n",
       "769999               -73.990093           504.0           1 Ave & E 16 St   \n",
       "\n",
       "        end station latitude  end station longitude  bikeid    usertype  \\\n",
       "0                  40.764618             -73.987895   17034  Subscriber   \n",
       "1                  40.719116             -74.006667   19847    Customer   \n",
       "2                  40.733143             -73.975739   19195  Subscriber   \n",
       "3                  40.688226             -73.979382   20195    Customer   \n",
       "4                  40.702551             -73.989402   19606  Subscriber   \n",
       "...                      ...                    ...     ...         ...   \n",
       "769995             40.750977             -73.987654   38486  Subscriber   \n",
       "769996             40.719156             -73.948854   35125    Customer   \n",
       "769997             40.720196             -73.989978   34112  Subscriber   \n",
       "769998             40.726218             -73.983799   14545  Subscriber   \n",
       "769999             40.732219             -73.981656   34991  Subscriber   \n",
       "\n",
       "       birth year  gender  year  month  \n",
       "0          1969.0       1  2013      6  \n",
       "1             NaN       0  2013      6  \n",
       "2          1965.0       2  2013      6  \n",
       "3             NaN       0  2013      6  \n",
       "4          1957.0       1  2013      6  \n",
       "...           ...     ...   ...    ...  \n",
       "769995       1978       1  2019     10  \n",
       "769996       1969       0  2019     10  \n",
       "769997       1986       1  2019     10  \n",
       "769998       1991       2  2019     10  \n",
       "769999       1955       2  2019     10  \n",
       "\n",
       "[770000 rows x 17 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the saved dataset (FINAL_SAMPLED_DATA.csv) from zip file here\n",
    "with zipfile.ZipFile('./FINAL_SAMPLED_DATA.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "    csv_filename = zip_ref.namelist()[0]\n",
    "bikes_df = pd.read_csv(csv_filename)\n",
    "bikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis, Hypothesis Testing and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight and Policy Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
