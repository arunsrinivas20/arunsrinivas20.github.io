{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Citibike Data in NYC\n",
    "\n",
    "## Andrew Li, Arun Srinivas, and Matt Wong\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The objective of this project is to analyze a large random sample of Citibike data from the years 2013 to 2019. We will look at different trends between location and month, as well as trends in the type of users and so much more. We will also try to build models that can potentially classify some attribute of a ride. Hopefully after reading this tutorial, you will not only be able to understand more about different statistics/trends about the Citibike data, but also how much Citibike has become integrated in the lives of New Yorkers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python Modules\n",
    "The main python modules we will be using in our project are:\n",
    "1. folium\n",
    "2. requests\n",
    "3. pandas\n",
    "4. numpy\n",
    "5. datetime\n",
    "6. statistics\n",
    "7. stats_models\n",
    "8. matplotlib\n",
    "9. sklearn\n",
    "10. urllib\n",
    "11. zipfile\n",
    "12. warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install folium\n",
    "\n",
    "import requests # for get request\n",
    "import pandas as pd # pandas\n",
    "import numpy as np # numpy array operations\n",
    "from datetime import datetime # datetime objects\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "import folium # for adding attributes to maps\n",
    "import urllib # for retrieving the files from the site\n",
    "import zipfile # for extracting and compressing the csv files\n",
    "import statistics # for help with standardizing data\n",
    "from folium.plugins import HeatMap # for visualizing a heat map for some attributes\n",
    "import statsmodels.formula.api as smf # for modeling with logistic regression\n",
    "import statsmodels.api as sm # for modeling with logistic regression\n",
    "from sklearn.model_selection import train_test_split # for generating training and testing data splits\n",
    "from sklearn.model_selection import GridSearchCV # for performing k-fold cross validation\n",
    "from sklearn.neighbors import KNeighborsClassifier # for modeling with k-nearest neighbors\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "This is the data collection phase of the data life cycle. During this phase, our focus is on collecting and transforming the data into a usable form. In our case, that usable form is a pandas dataframe derived from a CSV file containing the original data. <br> <br>\n",
    "\n",
    "After doing a lot of research for a dataset, we started to look towards the travel industry for motivation. We were motivated by data that contained information regarding ride sharing. At the end, we finalized our search on publicly available bike sharing data from Citi Bike NYC. We retrieved the data from the following site: https://s3.amazonaws.com/tripdata/index.html?fbclid=IwAR3BJ9yWYcKtBRYUMQq2SI1IZR9AnFel3C-eTato4mWXtwBz4olhrdrai7Js\n",
    "\n",
    "The company had available several months worth of data, ranging from June of 2013 to October of 2019, so we decided to explore each of these months. In attempting to utilize the data from the entire data range mentioned, we needed to perform several steps to automate the download and collection process. The following process will describe how we went about collecting and manipulating the data into one aggregate dataframe containing a certain number of entries of each date. <br>\n",
    "\n",
    "To collect and store the data, we used the following libraries from above:\n",
    "1. urllib\n",
    "2. zipfile\n",
    "3. pandas\n",
    "\n",
    "As mentioned before, the data was organized by year, and then month, so it was quite predictable in terms of figuring out how to extract the data files from the site. However, there were some unique aspects that required some workarounds. We looped through each year and month pair from June of 2013 to October of 2019. All of the month numbers in the file names had 2 digits, so we need to make sure all months before October (10th month) were **prepended with a \"0\"**. Most of the zip files followed the format **\"{date}{month_2_digit}-citibike-tripdata.zip\"**. The remaining zip files had a slightly different name format of **\"{date}{month_2_digit}-citibike-tripdata.csv.zip\"**. To get work around this, we instituted a check in the loop to determine if the current year was after 2016. If so, then the url and zip file name would be updated accordingly. \n",
    "\n",
    "Once those alterations were completed, as necessary, we proceeded to download the file data. To do so, we utilized the library function **urllib.request.urlretrieve** to download the zip file, indicated by the corresponding url, to the corresponding file path location, which was the current directory in this case. With the zip file downloaded, we proceeded to extract the contents of the zip file, only one CSV file in this case, using zipfile **extractall** method and extracted it to the current directory. We also retrieved the name of the file using the zipfile **namelist** and indexing at 0, as there is always only one element (CSV file) in said list. <br>\n",
    "    \n",
    "With the CSV data file download and extracted from the corresponding zip file, we transferred it into a pandas dataframe object. We made sure to add year and moth column to differentiate these data points from entries in future CSV files. After that, we decided to add only a sample of the resulting dataframe to our aggregate dataframe. This was due to the fact there were too many entries in every CSV file, so it would be impractical and inefficient to collect every data from each CSV for exploratory data analysis. With that in mind, we decided to randomly sample 10000 rows from each dataframe to be used for data analysis using the pandas **sample** method without replacement. In sampling with n=10000, we get a good representation of the data for that specific date.<br>\n",
    "    \n",
    "One issue we ran into when generating each sampled dataframe was with the column (attribute) names. We realized that, after approximately 2017, the attribute names were capitalized. Due to this difference, the resulting aggregate dataframe at first contained many NaNs because pandas concatenates dataframes based on their column names. The dataframe resulted in having 2 times the normal amount of column names: half being lowercase and half being capitalized. Ignoring case, the column names across all CSV files were identical. To work around this, we decided to create a list of default, lowercase column names, **col_names**, and replaced the column names of each sampled dataframe with our list. <br>\n",
    "    \n",
    "After iterating over each date, we ended up with a **list of 77 dataframes**, each corresponding to a specific date from the data. Since, we made sure that all of these dataframes had the same column names, we proceeded to concatenate all of them into one aggregate dataframe, as mentioned before. With 77 dataframes, each with 10000 randomly sampled data points, the new aggregate dataframe contained **770001** total data points. To do this, we utilized the pandas **concat** function, because it we could pass an iterable, which, in this case, was our list of sampled dataframes. The resulting dataframe was returned as output for display purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The data begins starting on June of 2013. \n",
    "year = 2013\n",
    "month = 6\n",
    "\n",
    "# In some of the csv files, we found that the column names differed by \n",
    "# capitalization, so below is a standard list of the all of the column\n",
    "# names that we apply to each sampled dataframe. \n",
    "col_names = ['tripduration', 'starttime', 'stoptime', 'start station id',\n",
    " 'start station name', 'start station latitude', 'start station longitude',\n",
    " 'end station id', 'end station name', 'end station latitude',\n",
    " 'end station longitude', 'bikeid', 'usertype', 'birth year', 'gender', 'year',\n",
    " 'month']\n",
    "\n",
    "# A list to hold all of the dataframes\n",
    "list_dfs = []\n",
    "\n",
    "# The last csv file contains the data recorded on October of 2019. The\n",
    "# loop will increment by 1 month, until after this date is reached. \n",
    "while (not (year == 2019 and month == 11)):\n",
    "    date_str = f'{year}{month}'\n",
    "    \n",
    "    # If the month is before October (10), prepend the corresponding \n",
    "    # integer value with a 0 to make it 2 digits\n",
    "    if month < 10:\n",
    "        date_str = f'{year}0{month}'\n",
    "    \n",
    "    # Corresponding download URL for each relevant zip file\n",
    "    url = f'https://s3.amazonaws.com/tripdata/{date_str}-citibike-tripdata.zip'\n",
    "    # Corresponding name for each relevant zip file\n",
    "    zip_file = f'{date_str}-citibike-tripdata.zip'\n",
    "    \n",
    "    # Format of zip file name changes after 2016, so this if stmt accounts for that\n",
    "    if (year > 2016):\n",
    "        url = f'https://s3.amazonaws.com/tripdata/{date_str}-citibike-tripdata.csv.zip'\n",
    "        zip_file = f'{date_str}-citibike-tripdata.csv.zip'\n",
    "    \n",
    "    # Zip file path points to the current directory \".\"\n",
    "    zip_file_path = f'./{zip_file}'\n",
    "    \n",
    "    print(year, month)\n",
    "    \n",
    "    # Download the corresponding zip file\n",
    "    urllib.request.urlretrieve(url, zip_file)\n",
    "    \n",
    "    csv_filename = ''\n",
    "    \n",
    "    # Extract the csv from the zip file and get its name\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "        csv_filename = zip_ref.namelist()[0]\n",
    "    \n",
    "    # Read the csv file into a pandas dataframe\n",
    "    df = pd.read_csv(f'./{csv_filename}')\n",
    "    \n",
    "    # Add the year and month attributes to distinguish from entries\n",
    "    # from entries from other dates\n",
    "    df['year'] = year\n",
    "    df['month'] = month  \n",
    "    \n",
    "    # Take a random sample of 10000 data points from the csv\n",
    "    sampled_df = df.sample(10000, replace=False, random_state=1)  \n",
    "    \n",
    "    # Make sure that all of the dataframes have the same attribute formats\n",
    "    sampled_df.columns = list(col_names)    \n",
    "    print(sampled_df.columns.values)\n",
    "    \n",
    "    # Add the dataframe to a list containing all dataframes \n",
    "    list_dfs.append(sampled_df)   \n",
    "    \n",
    "    # Increment by 1 month. Increment the year by 1 if month goes past Dec.\n",
    "    month += 1  \n",
    "    if (month > 12):\n",
    "        month = 1\n",
    "        year += 1\n",
    "\n",
    "# Concatenate all dataframes together to form a cumulative dataframe \n",
    "bikes_df = pd.concat(list_dfs)     \n",
    "bikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dataframe containing sampled observations from every date in the range of June of 2013 to October of 2019, we decided to export it to a CSV file for future use by using the pandas **to_csv** method. The reason behind this was because the process of downloading, extracting, and converting the data from the site was time and memory intensive. Another related thing to note was that kernel did die on occasion and had to restart. For this reason, rather than repeatedly performing these operations every time, we exported to the resulting dataframe to a CSV for efficiency purposes. It will later be converted back to a dataframe object when we begin our data analysis below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a csv file for later use and safe keeping\n",
    "bikes_df.to_csv('./FINAL_SAMPLED_DATA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the saved dataset (FINAL_SAMPLED_DATA.csv) from zip file here\n",
    "with zipfile.ZipFile('./FINAL_SAMPLED_DATA.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "    csv_filename = zip_ref.namelist()[0]\n",
    "bikes_df = pd.read_csv(csv_filename)\n",
    "bikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explain what each column in the data set means. It is explained in the list below: \n",
    "1. 'tripduration': the bike trip duration measured in seconds\n",
    "2. 'starttime': the start time of the trip \n",
    "3. 'stoptime': the stop time of the trip \n",
    "4. 'start station id': the id of the starting Citibike station\n",
    "5. 'start station name': the name of the starting Citibike station\n",
    "6. 'start station latitude': the latitude of the starting Citibike station\n",
    "7. 'start station longitude': the longitude of the starting Citibike station\n",
    "8. 'end station id': the id of the ending Citibike station\n",
    "9. 'end station name': the name of the ending Citibike station\n",
    "10. 'end station latitude': the latitude of the ending Citibike station\n",
    "11. 'end station longitude': the longitude of the ending Citibike station\n",
    "12. 'bikeid': the id of the bike\n",
    "13. 'usertype': customer => 24-hour pass or 3-day pass user; subscriber => Annual Member\n",
    "14. 'birth year': the birth year of the user \n",
    "15. 'gender': 0 => Unknown; 1 => Male; 2 => Female\n",
    "\n",
    "You can read more about Citibike and the data here:\n",
    "\n",
    "https://www.citibikenyc.com/system-data?fbclid=IwAR16Bs8wn3wTthMKWMLYDqfoRShF6BzVZMFPCk665ddfJdmgC3hJJ0mP7tw\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we used the following library from above:\n",
    "1. pandas\n",
    "\n",
    "The first thing we wanted to do is remove any incomplete data, especially those that had NaN for locations. For our heatmaps later on, we decided to construct them on the basis of start and end latitudes and longitudes. To do this, we utilizied the pandas **dropna** method to drop all rows that contained NaNs. We felt that this was safer than trying to impute values because, for example, it would be infeasible to replace unknown location values.\n",
    "\n",
    "In addition, we noticed that several of the years had **\"\\N\"** as an entry, which indicates that the year in the record is unknown. As a result, we resolved to drop those rows from the data, as we were planning on using those values in the data analysis later on. We only included rows that did not have **\"\\N\"** as a value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_df = bikes_df.dropna()\n",
    "\n",
    "bikes_df = bikes_df[bikes_df['birth year'] != '\\\\N']\n",
    "\n",
    "bikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first turn the starttime and stoptime column values into datetime objects. This will help us later on in the analysis when we need to access individual elements of the time (hour, day, year etc.). The format of the datetime object will be 'year-month-day hour:minute:second:millisecond'. We will be using Python's build-in function 'to_datetime' to help us convert the strings into datetime objects.\n",
    "\n",
    "In addition, we noticed that some of the data for the years was stored as an object or float, rather than just an int. As a result, we made sure to convert all year values to integers, so that we could use it for later reference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert starttime column values into datetime objects\n",
    "bikes_df['starttime'] = pd.to_datetime(bikes_df['starttime'])\n",
    "\n",
    "# Convert stoptime column values into datetime objects\n",
    "bikes_df['stoptime'] = pd.to_datetime(bikes_df['stoptime'])\n",
    "\n",
    "# Convert birth year to integer\n",
    "bikes_df['birth year'] = pd.to_numeric(bikes_df['birth year'], downcast='integer')\n",
    "bikes_df['year'] = pd.to_numeric(bikes_df['year'], downcast='integer')\n",
    "\n",
    "# Print the resulting dataframe\n",
    "bikes_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we used the following libraries from above:\n",
    "\n",
    "1. pandas\n",
    "2. datetime\n",
    "3. matplotlib\n",
    "4. folium\n",
    "5. numpy\n",
    "\n",
    "We will be plotting our data and analyzing trends that may be significant. Some plots you will see are most popular starting points of trips, comparison between subscriber and customer of Citibikes, the day that most people use the Citibikes, an analysis on gender of the user, and so much more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start off by plotting two simple graphs which will help with some analysis later on. The plots will be Frequency of Start Time of Trip and The Most Popular Days of the Week to ride a Citibike.\n",
    "\n",
    "### Plotting Frequency of Start Time of Trip\n",
    "We will be plotting the number of occurrences during the day when people begin their trip. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_day = {}\n",
    "\n",
    "# Fill time_of_day hash table with 0 occurrences of users for each hour\n",
    "for i in range(0, 24):\n",
    "    time_of_day[i] = 0\n",
    "\n",
    "# Loop through dataframe and increase counts of users by one \n",
    "# for that hour if the user started the trip at that hour\n",
    "for index, row in bikes_df.iterrows():\n",
    "    datetime_obj = row['starttime']\n",
    "    time_of_day[datetime_obj.hour] += 1\n",
    "\n",
    "# We will now plot histogram of the time of day people start using the bikes.\n",
    "plt.bar(time_of_day.keys(), time_of_day.values(), align='center')\n",
    "plt.xlabel('Hour in Day (0-23)')\n",
    "plt.ylabel('Number of Users Starting Trip')\n",
    "plt.title('Hour in Day vs. Number of Users Starting Trip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the graph above, most of the people rent Citibikes around 8am and 5pm. The reason for these two peaks at these two times could be that these two times correspond to when most people are going to work and leaving work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Most Popular Days of the Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "week_day= {}\n",
    "\n",
    "# Fill week_day hash table with 0 occurrences of users for each hour\n",
    "for i in range(0, 7):\n",
    "    week_day[i] = 0\n",
    "\n",
    "# Loop through dataframe and increase counts of users by one \n",
    "# for that hour if the user started the trip at that hour\n",
    "for index, row in bikes_df.iterrows():\n",
    "    datetime_obj = row['starttime']\n",
    "    week_day[datetime_obj.weekday()] += 1\n",
    "\n",
    "# We will now plot histogram of the time of day people start using the bikes.\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plt.bar(week_day.keys(), week_day.values(), align='center')\n",
    "plt.xticks(list(week_day.keys()), days, rotation=35)\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Number of Users Starting Trip')\n",
    "plt.title('Day of the Week vs. Number of Users Starting Trip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Source/Destination Stations\n",
    "Now, we want to try to find some trend and information about the Citibike stations in NYC. We will first try plotting heatmaps to see if we can find something interesting and then later on, we will plot the top Citibike stations that people go to for their bikes. By plotting these graphs, it will help us learn more about the location of Citibike stations as well as the most popular stations in NYC. \n",
    "\n",
    "#### Heatmaps of Source/Destination Stations\n",
    "First, we will be plotting the heatmap of source and destination locations. By plotting this, we will hopefully be able to analyze stations where most users retrieve and dock their bikes at in NYC. The brighter an area is (more orange/yellow) the more users that were in that area retrieving/docking their bikes. Feel free to zoom in too to take a closer look at the different areas in the city where most people are using Citibikes. \n",
    "\n",
    "The following is the Source Stations Heatmap: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below will plot the heatmap of source locations \n",
    "map_osm = folium.Map(location=[40.7358, -73.9760], zoom_start=11.5)\n",
    "\n",
    "# Create HeatMap of bike trip starting points in NYC\n",
    "bikes_df['count'] = 1\n",
    "HeatMap(data=bikes_df[['start station latitude', 'start station longitude', 'count']]\n",
    "        .groupby(['start station latitude', 'start station longitude'])\n",
    "        .sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(map_osm)\n",
    "\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the Destination Stations HeatMap: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below will plot the heatmap of destination locations \n",
    "map_osm = folium.Map(location=[40.7358, -73.9760], zoom_start=11.5)\n",
    "\n",
    "# Create HeatMap of bike trip ending points in NYC\n",
    "bikes_df['count'] = 1\n",
    "HeatMap(data=bikes_df[['end station latitude', 'end station longitude', 'count']]\n",
    "        .groupby(['end station latitude', 'end station longitude'])\n",
    "        .sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(map_osm)\n",
    "\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the two maps above, at first glance, we can see that most people start their trips in the heart of Manhattan and Brooklyn. If we zoom in more, we can get a greater sense of where most people start/end their trips from. In the source destination heatmap, we can that around 45th street and the southernest part of Manhattan, there is a good amount of trips that start from the stations near there. Likewise, we can see the same trend in the destination heatmap. Even though the heatmap can tell us which areas in NYC have a lot of people using the Citibikes, we cannot really infer too much about what is happening in those areas. We need to plot the most popular starting/ending stations and see them on the map. By doing this, we can single out the most popular stations in NYC and hopefully understand why these areas on the heatmap are so concentrated.   \n",
    "\n",
    "#### Plotting Popular Source/Destination Stations\n",
    "Instead of plotting a heatmap, we can plot the top 10 source stations as well as the top 10 destination stations from all the years 2013-2019. By plotting this, we can see more about where these stations are on the map and see if they correspond to the high concentration of users on the heatmap above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The two hashes below will store the key as the station name and \n",
    "# the value will be the number of occurrences that the key has appeared\n",
    "# in the dataframe\n",
    "# stations_start will store the source station counts\n",
    "# stations_end will store the destination station counts\n",
    "stations_start = {}\n",
    "stations_end = {}\n",
    "\n",
    "# Loop through dataframe and keep count of the number of people who leave \n",
    "# from the stations and arrive at the stations\n",
    "for index, row in bikes_df.iterrows():\n",
    "    station_name_start = row['start station name']\n",
    "    station_name_end = row['end station name']\n",
    "    \n",
    "    if station_name_start not in stations_start:\n",
    "        stations_start[station_name_start] = 0\n",
    "        \n",
    "    if station_name_end not in stations_end:\n",
    "        stations_end[station_name_end] = 0\n",
    "        \n",
    "    stations_start[station_name_start] += 1\n",
    "    stations_end[station_name_end] += 1\n",
    "\n",
    "# Sort the hash by value and retrieve top 10 source/destination stations\n",
    "# By top 10, we mean the stations that have the most users arriving/departing\n",
    "# from there\n",
    "stations_start = sorted(stations_start.items(), key=lambda x:-x[1])[:10]\n",
    "stations_end = sorted(stations_end.items(), key=lambda x:-x[1])[:10]\n",
    "\n",
    "# Extract first 10 highest items from each hash\n",
    "# and fill up the x and y lists that we will plot\n",
    "top_x_start = []\n",
    "top_y_start = []\n",
    "top_x_end = []\n",
    "top_y_end = []\n",
    "for key, value in stations_start:\n",
    "    top_x_start.append(key)\n",
    "    top_y_start.append(value)\n",
    "    \n",
    "for key, value in stations_end:\n",
    "    top_x_end.append(key)\n",
    "    top_y_end.append(value)\n",
    "\n",
    "# We will now plot histogram of the top 10 stations that people start and end at\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar(top_x_start, top_y_start, align='center')\n",
    "ax1.set_xlabel('Station Names')\n",
    "ax1.set_xticklabels(top_x_start, rotation = 45, ha='right')\n",
    "ax1.set_ylabel('Number of Users Starting Trip at that Station')\n",
    "ax1.set_title('Top 10 Source Stations with Most Users')\n",
    "\n",
    "fig, ax2 = plt.subplots()\n",
    "ax2.bar(top_x_end, top_y_end, align='center')\n",
    "ax2.set_xlabel('Station Names')\n",
    "ax2.set_xticklabels(top_x_end, rotation = 45, ha='right')\n",
    "ax2.set_ylabel('Number of Users Ending Trip at that Station')\n",
    "ax2.set_title('Top 10 Destination Stations with Most Users')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From observation, it looks like that the top 10 most popular stations are the same (except in different order) across both graphs. Now since, we know the most popular stations, we will also try to analyze the age of the people who go to these popular stations. This will help us understand more about who uses these bikes in these areas. We will output graphs of age groups in these popular stations. The age groups will be teens: 0-18, young adults: 18-36, middle-aged adults:36-55, and older adults: 55+. The lower bound on the age is inclusive while the upper bound on the age is exclusive for that range.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add new age column calculated by the current dataset year \n",
    "# minus the birth year of the user\n",
    "bikes_df['age'] = bikes_df['year']-bikes_df['birth year']\n",
    "\n",
    "# The variables below will correspond to the subplot matrix \n",
    "# where i represents the row and j represents the column\n",
    "i = 0\n",
    "j = 0\n",
    "\n",
    "# We will be creating 10 subplots where each plot represents the\n",
    "# amount of users at that station vs. the age range (defined above) \n",
    "fig, ax1 = plt.subplots(5, 2, figsize=(20, 40))\n",
    "\n",
    "# For each popular station\n",
    "for name in top_x_start:\n",
    "    # Retrieve subset of dataframe where the starting station name matches the \n",
    "    # current station name we are making plot for    \n",
    "    current_subset = bikes_df.loc[bikes_df['start station name'] == name]\n",
    "    \n",
    "    # Initialize hash where key is the age group and the value is the count for each \n",
    "    # age group for the current station we are making plot for     \n",
    "    age_hash_start = {'teens':0, 'young adults': 0, 'middle-aged adults': 0, 'older adults': 0}\n",
    "    \n",
    "    # Load hash\n",
    "    for index, row in current_subset.iterrows():\n",
    "        user_age = row['age']\n",
    "        if user_age < 18:\n",
    "            age_hash_start['teens'] += 1\n",
    "        elif user_age < 36:\n",
    "            age_hash_start['young adults'] += 1\n",
    "        elif user_age < 55:\n",
    "            age_hash_start['middle-aged adults'] += 1\n",
    "        else:\n",
    "            age_hash_start['older adults'] += 1\n",
    "            \n",
    "    # Retrieve subset of dataframe where the ending station name matches the \n",
    "    # current station name we are making plot for    \n",
    "    current_subset = bikes_df.loc[bikes_df['end station name'] == name]\n",
    "    \n",
    "    # Reset age group hash because now we will be treating the current station\n",
    "    # name as the destination station\n",
    "    age_hash_end = {'teens':0, 'young adults': 0, 'middle-aged adults': 0, 'older adults': 0}\n",
    "    \n",
    "    # Load hash\n",
    "    for index, row in current_subset.iterrows():\n",
    "        user_age = row['age']\n",
    "        if user_age < 18:\n",
    "            age_hash_end['teens'] += 1\n",
    "        elif user_age < 36:\n",
    "            age_hash_end['young adults'] += 1\n",
    "        elif user_age < 55:\n",
    "            age_hash_end['middle-aged adults'] += 1\n",
    "        else:\n",
    "            age_hash_end['older adults'] += 1\n",
    "    \n",
    "    # Graph the number of users for each age group for current station. Each age group \n",
    "    # will have two bars: one bar will be the number of users treating the current \n",
    "    # station as a start station and the other bar will be the number of users treating\n",
    "    # the current station as an end station\n",
    "    ind = np.arange(4) \n",
    "    ax = ax1[i % 5, j % 2]\n",
    "    ax.bar(ind, list(age_hash_start.values()), .35, label='Source Station', align='center')\n",
    "    ax.bar(ind+.35, list(age_hash_end.values()), .35, label='Destination Station', align='center')\n",
    "    ax.set_xlabel('Age Range')\n",
    "    ax.set_xticks(ind+.35/2)\n",
    "    ax.set_xticklabels(list(age_hash_end.keys()))\n",
    "    ax.set_ylabel('Number of Users')\n",
    "    ax.set_title('Age range of Users at ' + name + ' Station')\n",
    "    ax.legend()\n",
    "    i += 1\n",
    "    j += 1\n",
    "\n",
    "# Format plots to make it look nicer\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(hspace=0.3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the 10 graphs comparing the number of users to the age group for each popular station, we can see that age groups fluctuate in different stations. For example, in Pershing Square North, we can see that the dominating age group is middle-aged adults while in Lafayette St & E 8 St, the dominating age group is young-adults. Keep this in mind as it will help in our analysis. \n",
    "\n",
    "Now we can see what the most popular stations are in the histogram as well as the age groups that appear most at these stations, but now let's see where they are on the map. By doing this, we can look at the stations and try to analyze the environment around that station so that we can understand why that station is so popular. Since the top 10 source and destination stations are the same, we will only provide one map. If you click on a point on the map, it will display the name of the Citibike station. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_osm = folium.Map(location=[40.7300, -74.0007], zoom_start=13)\n",
    "\n",
    "# Add points to map to show most popular source stations\n",
    "for name in top_x_start:\n",
    "    # Retrieve the latitude and longitude of that location in dataframe     \n",
    "    row = bikes_df.loc[bikes_df['start station name'] == name].iloc[0]\n",
    "    lat = row['start station latitude']\n",
    "    long = row['start station longitude']\n",
    "    \n",
    "    # Add a circle indicating the location in NYC\n",
    "    folium.Circle(\n",
    "        radius=20,\n",
    "        location=[lat, long],\n",
    "        popup=name,\n",
    "        color='black',\n",
    "        fill=True,\n",
    "    ).add_to(map_osm)\n",
    "\n",
    "map_osm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the graph above, we can see that all of the top 10 stations are located in Manhattan. Also, we can observe that most of these locations are located near very busy bus/metro stations. For example, we can see that the most popular start station (Pershing Square North) is located right next to Grand Central Terminal which is one of the major metro stations in NYC. Likewise, there are Citibike stations right next to Port Authority Bus Terminal and 34th Street Penn Station. These two bus/metro stations are also very busy and popular stations for New Yorkers. Looking at our age range graphs for these stations, we can see that they are mostly middle-aged adults. We could probably infer that most Citibike users in this area use their bike to get to these stations either for work or travel or they leave from these stations after coming back from work. \n",
    "\n",
    "You can read more about Grand Central Terminal here:\n",
    "https://www.grandcentralterminal.com/about/\n",
    "\n",
    "You can read more about Penn Station here:\n",
    "https://en.wikipedia.org/wiki/Pennsylvania_Station_(New_York_City)\n",
    "\n",
    "If we look at the downtown stations, Lafayette St & E 8 St and Broadway & E 14 St, we can see that these Citibike stations are near NYU, a university. By looking at our age range graphs, we can observe that most of the users, are young adults, college students and people who are just starting their careers. If we look at the two stations above those, Broadway & E 22 St and W 21 St & 6 Ave, we can see that those are located near/in the Flatiron District. Baruch College is in the area as well. Therefore, we can see a lot of college students/young adults in this area. We could probably infer that most Citibike users in this area are college students who use the bikes to get to class, dorms, and even restaurants.   \n",
    "\n",
    "You can read more about Flatiron District here:\n",
    "https://en.wikipedia.org/wiki/Flatiron_District\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Habits Between Subscribers and Customers (day-pass and single-ride users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we used the following libraries from above:\n",
    "1. pandas\n",
    "2. datetime\n",
    "3. matplotlib\n",
    "4. statistics\n",
    "5. folium\n",
    "6. numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = bikes_df.loc[bikes_df['usertype'] == \"Customer\"]\n",
    "subscribers = bikes_df.loc[bikes_df['usertype'] == \"Subscriber\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customer vs Subscriber Frequency By Day of Week\n",
    "\n",
    "TODO: Standardize for better comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare day of week,\n",
    "week_day_sub= {}\n",
    "week_day_cust= {}\n",
    "\n",
    "# Fill time_of_day hash table with 0 occurrences of users for each hour\n",
    "for i in range(0, 7):\n",
    "    week_day_sub[i] = 0\n",
    "    week_day_cust[i] = 0\n",
    "\n",
    "# Loop through dataframe and increase counts of users by one \n",
    "# for that hour if the user started the trip at that hour\n",
    "for index, row in bikes_df.iterrows():\n",
    "    datetime_obj = row['starttime']\n",
    "    if row['usertype'] == 'Customer':\n",
    "        week_day_cust[datetime_obj.weekday()] += 1\n",
    "    elif row['usertype'] == 'Subscriber':\n",
    "        week_day_sub[datetime_obj.weekday()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar( week_day_sub.keys(), week_day_sub.values(), label = 'Subscribers')\n",
    "plt.bar( week_day_cust.keys(), week_day_cust.values(), label = 'Non-subscribers')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1))\n",
    "\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.xticks(list(week_day_cust.keys()), days, rotation=35)\n",
    "plt.ylabel('Number of Users Starting Trip')\n",
    "plt.title('Day of the Week vs. Number of Users Starting Trip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, we can see several notable trends: \n",
    "1. Subscriber usage heavily outweighs non-subscriber usage.\n",
    "2. Subscriber usage is at its highest during business days (Monday - Friday) and falls off on the weekends.\n",
    "3. Non-subscriber usage peaks on the weekends (Saturday-Sunday) and falls off during business days. \n",
    "\n",
    "To make comparisons between subscribers and non-subscribers more easily, we need to standardize both datasets. To normalize the data, we will be computing the Z score for each the subscriber and non-subscriber datasets using this formula: \n",
    "\n",
    "Z = (X - mean) / stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_wd_sub = statistics.mean(list(week_day_sub.values()))\n",
    "stdev_wd_sub = statistics.stdev(list(week_day_sub.values()))\n",
    "std_wd_sub_vals = [(x - mean_wd_sub)/stdev_wd_sub for x in list(week_day_sub.values())]\n",
    "\n",
    "mean_wd_cust = statistics.mean(list(week_day_cust.values()))\n",
    "stdev_wd_cust = statistics.stdev(list(week_day_cust.values()))\n",
    "std_wd_cust_vals = [(x - mean_wd_cust)/stdev_wd_cust for x in list(week_day_cust.values())]\n",
    "\n",
    "print(std_wd_sub_vals)\n",
    "print(std_wd_cust_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( list(week_day_sub.keys()), std_wd_sub_vals, label='Subscribers')\n",
    "plt.plot( list(week_day_cust.keys()), std_wd_cust_vals, label='Non-subscribers')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1))\n",
    "\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.xticks(list(week_day_cust.keys()), days, rotation=35)\n",
    "plt.ylabel('Number of Users Starting Trip (Standardized)')\n",
    "plt.title('Day of the Week vs. Number of Users Starting Trip Standardized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This standardized graph shows us that subscriber and non-subscriber usage has an inverse relationship:\n",
    "Through the business days, subscriber usage increases and non-subscriber usage decreases until wednesday. After Wednesday, subsccriber usage decreases and non-subscriber usage increases through the weekend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customer vs Subscriber Frequency By Time of Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare time of day\n",
    "\n",
    "# compare day of week,\n",
    "time_sub= {}\n",
    "time_cust= {}\n",
    "\n",
    "# Fill time_of_day hash table with 0 occurrences of users for each hour\n",
    "for i in range(0, 24):\n",
    "    time_sub[i] = 0\n",
    "    time_cust[i] = 0\n",
    "\n",
    "# Loop through dataframe and increase counts of users by one \n",
    "# for that hour if the user started the trip at that hour\n",
    "for index, row in bikes_df.iterrows():\n",
    "    datetime_obj = row['starttime']\n",
    "    if row['usertype'] == 'Customer':\n",
    "        time_cust[datetime_obj.hour] += 1\n",
    "    elif row['usertype'] == 'Subscriber':\n",
    "        time_sub[datetime_obj.hour] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(time_sub.keys()), list(time_sub.values()), label='Subscribers')\n",
    "plt.bar(list(time_cust.keys()), list(time_cust.values()), label='Non-subscribers')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1))\n",
    "plt.xlabel('Hour in Day (0-23)')\n",
    "plt.ylabel('Number of Users Starting Trip')\n",
    "plt.title('Hour in Day vs. Number of Users Starting Trip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph again shows that:\n",
    "1. Subscriber usage forms a bimodal distribution, peaking at 8 AM and 5-6 PM.\n",
    "2. Non-subscriber usage forms a unimodal distribution, peaking between 1-5 PM.\n",
    "\n",
    "However, the scales for subscribers and non-subscribers are again not the same, so we need to standardize the data as we did before using Z scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_hr_sub = statistics.mean(list(time_sub.values()))\n",
    "stdev_hr_sub = statistics.stdev(list(time_sub.values()))\n",
    "std_hr_sub_vals = [(x - mean_hr_sub)/stdev_hr_sub for x in list(time_sub.values())]\n",
    "\n",
    "mean_hr_cust = statistics.mean(list(time_cust.values()))\n",
    "stdev_hr_cust = statistics.stdev(list(time_cust.values()))\n",
    "std_hr_cust_vals = [(x - mean_hr_cust)/stdev_hr_cust for x in list(time_cust.values())]\n",
    "\n",
    "print(std_hr_sub_vals)\n",
    "print(std_hr_cust_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(time_sub.keys()), std_hr_sub_vals, label=\"Subscribers\")\n",
    "plt.plot(list(time_cust.keys()), std_hr_cust_vals, label=\"Non-subscribers\")\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1))\n",
    "plt.xlabel('Hour in Day (0-23)')\n",
    "plt.ylabel('Number of Users Starting Trip (Standardized)')\n",
    "plt.title('Hour in Day vs. Number of Users Starting Trip Standardized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph better shows how usage compares throughout the day between subscribers and non-subcribers. To reiterate the same conclusions as the non-standardized plot:\n",
    "1. Subscriber usage is low until around 7-9 AM where usage peaks, then decreaes at around 12-3 PM, then peaks again at 5-6 PM before finally falling off again (forming a bimodal distribution).\n",
    "2. Non-subsriber usage starts low and gradually increases until 2-4 PM where usage peaks, then slowly falls down to lower levels (forming a unimodal distribution.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap of Trips for Customers vs Subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_osm = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "\n",
    "bikes_df['count'] = 1\n",
    "HeatMap(data=customers[['end station latitude', 'end station longitude']]\n",
    "        .groupby(['end station latitude', 'end station longitude'])\n",
    "        .sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(map_osm)\n",
    "\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_osm = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "\n",
    "bikes_df['count'] = 1\n",
    "HeatMap(data=subscribers[['end station latitude', 'end station longitude']]\n",
    "        .groupby(['end station latitude', 'end station longitude'])\n",
    "        .sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(map_osm)\n",
    "\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this heatmap was to see if subscriber and non-subscriber usage was concentrated in specific areas (ie if subscriber usage was centered around office/work districts and if non-subscriber usage was centered around tourist attractions). It appears that end-stop locations between subscribers and non-subsribers is extremely similar. However, if you zoom in closer into the map, you can see that specific stops differ in 'heat'. For example, Penn Station and Grand Central are more popular stops for non-subscribers than it is for subscribers whereas the Spring St.-Lafayette Street stop is more popular for subscribers than it is for non-subscribers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most Popular Stops for Subscribers and Non-subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_pop = {}\n",
    "sub_pop_cust = {}\n",
    "\n",
    "cust_pop = {}\n",
    "cust_pop_sub = {}\n",
    "\n",
    "# Getting the frequency of each end stop sorted by most frequent\n",
    "cust_pop = customers['end station name'].value_counts()\n",
    "sub_pop = subscribers['end station name'].value_counts()\n",
    "\n",
    "#Standardizing data\n",
    "mean_sub = statistics.mean(list(sub_pop))\n",
    "stdev_sub = statistics.stdev(list(sub_pop))\n",
    "std_sub_vals = [(x,(sub_pop[x] - mean_sub)/stdev_sub) for x in sub_pop.keys()]\n",
    "\n",
    "mean_cust = statistics.mean(list(cust_pop))\n",
    "stdev_cust = statistics.stdev(list(cust_pop))\n",
    "std_cust_vals = [(x, (cust_pop[x] - mean_cust)/stdev_cust) for x in cust_pop.keys()]\n",
    "\n",
    "# Getting the NON-subscriber frequency of the top 10 subscriber stops\n",
    "for tup in std_cust_vals[:10]:\n",
    "    cust_pop_sub[tup[0]] = list(filter(lambda x: tup[0] == x[0], std_sub_vals))[0][1]\n",
    "\n",
    "# Getting the SUBSCRIBER frequency of the top 10 NON-subscriber stops\n",
    "for tup in std_sub_vals[:10]:\n",
    "    sub_pop_cust[tup[0]] = list(filter(lambda x: tup[0] == x[0], std_cust_vals))[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "N = 10\n",
    "ind = np.arange(N) \n",
    "\n",
    "ax.bar(ind, [x[1] for x in std_sub_vals[:10]], width = 0.35, label='subscribers', align='center', color = 'b')\n",
    "ax.bar(ind + 0.35, list(sub_pop_cust.values()), width = 0.35, label='non-subscribers', align='center', color = 'r')\n",
    "plt.xticks(ind + 0.35 / 2, list(sub_pop[:10].keys()), rotation = 45, ha='right')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1))\n",
    "plt.xlabel('Stop Destination Name)')\n",
    "plt.ylabel('# of Users Ending their Destination at Stop')\n",
    "plt.title('Most Popular End Stops for Subscribers (compared to non-subcsribers)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart plots the frequency of the top 10 stops for Subscribers as well as the frequencies of the same stops for non-subscribers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "N = 10\n",
    "ind = np.arange(N) \n",
    "\n",
    "ax.bar(ind, [x[1] for x in std_cust_vals[:10]], width = 0.35, label='non-subscribers', align='center', color='r')\n",
    "ax.bar(ind + 0.35, list(cust_pop_sub.values()), width = 0.35, label='subscribers', align='center', color='b')\n",
    "plt.xticks(ind + 0.35 / 2, list(cust_pop[:10].keys()), rotation = 45, ha='right')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1))\n",
    "plt.xlabel('Stop Destination Name)')\n",
    "plt.ylabel('# of Users Ending their Destination at Stop Standardized')\n",
    "plt.title('Most Popular End Stops for Non-subscribers (compared to subcsribers)', y=1.18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chart plots the top 10 most popular stops for non-subscribers and compares it to the normalized popularity of subscribers.\n",
    "\n",
    "#### Analysis of the two charts:\n",
    "\n",
    "It appears that the most popular stops for subcribers are not the most popular for non-subcribers. Inversely, the most popular stops for non-subscribers are not the most popular stops for subscribers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Patterns within the Data\n",
    "\n",
    "The goal of this section is trying to see if we can determine patterns for gender. For example, one of the things we studied was the density of bikeshare start positions and see where each gender is most concentrated. We make conclusions based which areas tend to be the most concentrated by gender. The first thing we did was segregate the data by gender, making on dataframe for males and one for females. One thing to note is that there were 3 gender classifications, with 0 representing unknown. At first, we wanted to include it, as it could be representing genders other than males and females, but we realized that it may also mean that the gender attribute was purely unknown. As a result of this ambiguity, we resolve to exclude unknown from our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heatmap of Trips for Males and Females"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_df['count'] = 1\n",
    "males_df = bikes_df[bikes_df['gender'] == 1]\n",
    "females_df = bikes_df[bikes_df['gender'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code uses the folium **Map** object to initialize a map that is centered at New York City, just south of Manhattan. We then use the **HeatMap** object from the plugins module of folium and use the males dataframe from earlier. We plot the latitude and longitude of each coordinate together, using group_by and sum functions to combine the common observations, and then we add the HeatMap to our existing map and display it. The same was done for the feamle dataframe as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_osm_males = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "\n",
    "HeatMap(data=males_df[['start station latitude', 'start station longitude', 'count']]\n",
    "        .groupby(['start station latitude', 'start station longitude'])\n",
    "        .sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(map_osm_males)\n",
    "\n",
    "map_osm_males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_osm_females = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "\n",
    "HeatMap(data=females_df[['start station latitude', 'start station longitude', 'count']]\n",
    "        .groupby(['start station latitude', 'start station longitude'])\n",
    "        .sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(map_osm_females)\n",
    "\n",
    "map_osm_females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the maps above, we get some interesting observations at first glance. The maps are pretty similar, but there are some subtle differences. In general, we see that there highly dense (red) areas near Grand Central, 1st Avenue & East 14th Street, Chinatown, etc. This makes sense because Grand Central and Chinatown are especially tourist and travel (work) heavy, so we are bound to see heat map concetrate on those areas. On subtle difference we can see is that near the Brooklyn sports area, specifically the region encompassing Brookyln Nets stadium, it is slightly more concentrated with males than with females. This potentially indicates that males utilize bike share at a higher frequency for the past 6 years than do females when considering this region in Brooklyn. \n",
    "\n",
    "Other than this slight difference, which we do not think is very significant, male and female patterns of where they begin their bike share trips are almost identical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling: Creating Models to Predict the Gender of a Rider\n",
    "\n",
    "For this section, we used the following libraries from above:\n",
    "1. pandas\n",
    "2. sklearn\n",
    "3. statsmodels\n",
    "\n",
    "### Generating Models to Predict Gender\n",
    "\n",
    "The goal of this section is to figure out whether we can accurate predict the gender of a potential. As a result, we set out to produce a couple of models and compare them as well. \n",
    "\n",
    "Creating the heat maps helped us figure out how to determine the our predictors for the logistic regression model. The goal of our model was to see if we could predict the gender, male or female, based on a few factors regarding trip details. Originally, we wanted to include the latitude and longitude in the predictors, but we realized that it was hard to interpret that in the model. Latitude increases going from south to north, while longitude increases going west to east. Despite these facts, we recognized that, as we were centering on a specific region, New York City, there would be very little variation amongst the values. They were very close together, so, for example, a rider starting their trip at a longitude that was 0.01 greater than that of another rider did not seem to be that signifcant in predicting the gender of a rider. While this may not be convincing alone, our heatmaps above defintely confirm this observation. As the heat maps are nearly identical, we see that a difference in latitude and longitude will not be a great determinant in predicting the gender of a potential rider. \n",
    "\n",
    "As a result, we decided to create models for predicting gender, based on **starttime, duration, usertype, and age**.\n",
    "\n",
    "The first thing we will do is prepare our data to be modeled. This involves converting values, so that it can be easy to interpret. \n",
    "1. We first converted the **starttime** variable to seconds. To do so we extracted the time portion only from the datetime object and then converted it to seconds. This was because, as the data spanned over several years, the day of which the trip occurred did not matter for the purposes of modeling. We named the variable **startime_sec**. We summed the following:\n",
    "    - time.hour * 3600 => seconds\n",
    "    - time.minutes * 60 => seconds\n",
    "    - time.second <br>\n",
    "<br>\n",
    "2. We then calculated the age by using existing attributes. We simply subtracted the **year** of the trip from the **birth year** of the rider of the trip to calculate the age of the rider. We named the variable **age**.\n",
    "3. We converted the usertype variable to numerical format. While it being a string did not impact the logistic regression modeil, it did prevent the tree from being created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the start times to seconds\n",
    "bikes_df['starttime_sec'] = bikes_df['starttime'].map(lambda x: (x.hour * 3600) + (x.minute * 60) + x.second)\n",
    "\n",
    "# Calculate the age by subtracting birth year from year\n",
    "bikes_df['age'] = bikes_df['year'] - bikes_df['birth year']\n",
    "\n",
    "# Encode the categorical usertype_code as integers (1 = Subscriber and 0 = Customer)\n",
    "bikes_df['usertype_code'] = bikes_df['usertype'].map({'Subscriber': 1, 'Customer': 0})\n",
    "\n",
    "bikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second thing that we will do is normalize the data set. The reason for this is because we noticed that the **starttime_sec** and **duration** predictors were much larger in magnitude, compared to the rest of the predictors we chose. Another reason for doing so was because we wanted to make sure we could compare their impacts on predicting gender, relatively speaking. It normalizes the distances between the values, putting them on a similar scale.\n",
    "\n",
    "To do this, we made our own function to standardize a pandas Series object by computing the z-score of each data value. In other words, we took every data item for attribute and computed its z-score. \n",
    "<br><br>\n",
    "**z-score = (value - mean) / (standard deviation)**\n",
    "<br><br>\n",
    "We standardized all the predictors this way and created our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function that standardizes an input series object.\n",
    "# We essentially calculate the z-score for each value in the Series. \n",
    "def standardize_attr(series):\n",
    "    avg = series.mean()\n",
    "    stdv = series.std()\n",
    "    series_standardized = (series - avg) / stdv\n",
    "    return series_standardized\n",
    "\n",
    "# A list of all the attributes, including the response variable, gender\n",
    "attrs = ['starttime_sec', 'tripduration', 'age', 'usertype_code', 'gender']\n",
    "\n",
    "# Extract those relevant attributes from the main bikes_df\n",
    "norm_bikes_df = bikes_df[attrs]\n",
    "\n",
    "# Remove all rows where the gender is unknown (0)\n",
    "norm_bikes_df = norm_bikes_df[norm_bikes_df['gender'] != 0]\n",
    "\n",
    "# Code the gender such that 0 is Male and Female is 1\n",
    "norm_bikes_df['gender'] -= 1\n",
    "\n",
    "# Remove the last 2 attributes before standardizing as they are categorical\n",
    "# and need no standardization\n",
    "attrs.pop()\n",
    "attrs.pop()\n",
    "\n",
    "# For every continuous attribute, standardize the column of values\n",
    "for attr in attrs:\n",
    "    norm_bikes_df[attr] = standardize_attr(norm_bikes_df[attr])\n",
    "    \n",
    "norm_bikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third thing we will do is randomly section out data for training, validation, and test data. We will use a **75-25** split, which corresponds to 75% training data and 25% test data. Before this, we made sure sample out **100000** random observations from the data set. This was because the kernel kept dying when using all over 700000 observations. To do this, we will use the **train_test_split** function from the model_selection module of sklearn. As we are generating a couple of models, we want to be able to compare them on the same dataset, which will be our test data in this case. <br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample 1000000 attributes from our standardized dataframe\n",
    "norm_bikes_df = norm_bikes_df.sample(100000)\n",
    "\n",
    "y = norm_bikes_df['gender']\n",
    "\n",
    "# Get the training and testing data sets\n",
    "norm_bikes_train, norm_bikes_test, gender_train, gender_test = train_test_split(norm_bikes_df, y, test_size=0.25)\n",
    "\n",
    "print(f'The size of our training data is {len(norm_bikes_train)}.')\n",
    "print(f'The size of our test data is {len(norm_bikes_test)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bikes_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_bikes_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Now we will generate our model for logistic regression on training data set we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'gender'\n",
    "predictors = 'starttime_sec + tripduration + usertype_code + age'\n",
    "\n",
    "# Make the R-like logistic regression formula for glm\n",
    "formula = f'{response} ~ {predictors}'\n",
    "\n",
    "log_reg_model = smf.glm(formula = formula, data=norm_bikes_train, family=sm.families.Binomial())\n",
    "result = log_reg_model.fit()\n",
    "\n",
    "# Get the summary statistics\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the logistic regression model we can analyze the coefficients to see determine how they affect the classification of gender. Please note that repeated runs of the training and testing data set splitting, might yield different fits. When we fitted the model repeated times in this fashion, we saw that the majority of models had all significant predictors (near-zero p-values). In the few exceptions, we saw that starttime_sec and, rarely, tripduration were insignificant in explaining the response. We will assume, for the purposes of this analysis, that all the predictors were significant. We performed z-tests (null hypothesis is that the predictor is insignifcant, when accounting for other predictors, and the alternative hypothesis is that the predictor is significant, when accounting for other predictors) on all of them and found that their associated p-values were all less than a significance level of alpha=0.5 (rejected the null hypothesis each time). With the same reasoning, coefficients also change in every rerun of the model. With every model, we saw that intercept, starttime_sec, usertype_code, and age were all negative, while the coefficient of tripduration was positive. We will, as a result, assume that their signs stay the same for the purposes of this analysis. \n",
    "\n",
    "Each of the interpretions of the coefficients are as follows:\n",
    "\n",
    "1. **starttime_sec**: For every additional second later in the day, on average, the odds of a bike rider being classified as female decrease by factor of e^(-0.0214) = 0.979, while holding other predictors fixed. This coefficient is negative, which indicates that it generally decreases the probability of a rider being classified as female. However, the coefficient is near-zero in magnitude, indicating that it has little influence on the probability of a rider being classified as female. \n",
    "\n",
    "2. **tripduration**: For every additional second in trip duration, on average, the odds of a bike rider being classified as female increase by factor of e^(0.4521) = 1.572, while holding other predictors fixed. This coefficient is positive, which indicates that it generally increases the probability of a rider being classified as female. The coefficient is larger in magnitude, relative to the other coefficients, indicating that it has a large influence on the probability of a rider being classified as female. \n",
    "\n",
    "3. **usertype_code**: Being a Subscriber, on average, the odds of a bike rider being classified as female decrease by factor of e^(-0.4878) = 0.614, when compared to Customers, while holding other predictors fixed. This coefficient is negative, which indicates that it generally decreases the probability of a rider being classified as female. The coefficient is larger in magnitude, relative to the other coefficients, indicating that it has a large influence on the probability of a rider being classified as female. \n",
    "\n",
    "4. **age**: For every additional year, on average, the odds of a bike rider being classified as female increase by factor of e^(-0.1217) = 0.885, while holding other predictors fixed. This coefficient is negative, which indicates that it generally decreases the probability of a rider being classified as female. The coefficient is not as large when compared to (2) and (3) in magnitude, relative to the other coefficients, indicating that it has a decent influence on the probability of a rider being classified as female. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of Logistic Regression\n",
    "\n",
    "To test the accuracy of our model, we evaluated its performance on the test data set we sectioned out earlier. With those probabilites, we then classified each predicted observation, with above 0.5 being classified as female and below being classified as male. We then counted the number of matching observations and then divided that by the total number of observations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including a list of all the predictors\n",
    "features = ['starttime_sec', 'tripduration', 'usertype_code', 'age']\n",
    "\n",
    "# Predict using the test dataset\n",
    "gender_pred_probs = result.predict(norm_bikes_test[features])\n",
    "\n",
    "# Each prediction output a probability so finish the prediction\n",
    "# by classifying each one with a cutoff value of 0.5\n",
    "gender_pred = [1 if prob > 0.5 else 0 \n",
    "               for prob in gender_pred_probs]\n",
    "\n",
    "num_correct = 0\n",
    "pred_iter = 0\n",
    "\n",
    "# Find the number of correctly predicted observations\n",
    "for true_value in gender_test:\n",
    "    if true_value == gender_pred[pred_iter]:\n",
    "        num_correct += 1\n",
    "    pred_iter += 1\n",
    "    \n",
    "print(f'There were {num_correct} correctly predicted observations out of {len(gender_pred)}.')\n",
    "print(f'This yields a model prediction accuracy of about {100 * num_correct / len(gender_pred)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Nearest Neighbors (kNN)\n",
    "\n",
    "Now we will generate our k-nearest neigbors (kNN). We will use libraries to help with corss validation procedure, so that we can generate a tree with optimal number of neighbors. To do this we will utilize sklearn's **DecisionTreeClassifier** object to build the tree and sklearn's **GridSearchCV** object from the model_selection module to perform the 10-fold cross-validation procedure. We will perform this on the training data.\n",
    "\n",
    "From constructing the kNN model, we first had to tune the hyperparameter, in this case the number of neighbors. To do so, we performed a 10-fold cross validation procedure and tested parameters from 2 to 10, inclusive. We found that the optimal number of neighbors was 8. However, please note that this value may tune differently on repeated runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = 'gender'\n",
    "\n",
    "# Including a list of all the predictors\n",
    "features = ['starttime_sec', 'tripduration', 'usertype_code', 'age']\n",
    "\n",
    "# We are testing for the best hyperparameter between 2 and 10\n",
    "param_grid = {'n_neighbors': range(2,10)}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Perform the 10-fold CV procedure after we fit the model on the next line\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(X=norm_bikes_train[features], y=gender_train)\n",
    "\n",
    "# Get the best, most accurate model\n",
    "best_clf = grid.best_estimator_\n",
    "\n",
    "# Output the best value of k we selected\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance of kNN\n",
    "\n",
    "To test the accuracy of our model, we evaluated its performance on the test data set we sectioned out earlier. We then counted the number of matching observations and then divided that by the total number of observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the test dataset\n",
    "gender_pred = best_clf.predict(norm_bikes_test[features])\n",
    "\n",
    "num_correct = 0\n",
    "pred_iter = 0\n",
    "\n",
    "# Find the number of correctly predicted observations\n",
    "for true_value in gender_test:\n",
    "    if true_value == gender_pred[pred_iter]:\n",
    "        num_correct += 1\n",
    "    pred_iter += 1\n",
    "    \n",
    "print(f'There were {num_correct} correctly predicted observations out of {len(gender_pred)}.')\n",
    "print(f'This yields a model prediction accuracy of about {100 * num_correct / len(gender_pred)}%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From comparing the predictive capabilities of both of the logistic regression and kNN models, we found that both of them were quite accurate in predicting whether a potenital rider, given their age, duration of trip, start time of trip in the day, and user type. In general, these accuracies are bound to change, but we found that logistic regression did slightly better in its performance. We displayed the accuracy of each model on the test data set in the output of the above cells. \n",
    "\n",
    "From the logistic model, we can also determine which predictors are most important in predicting the gender of a rider. From the high coefficients of trip duration and user_type, we would say that these predictors are the most important and influential in predicting the gender of a rider. Being a Subscriber significantly decreases odds, on average, of predicting a rider to be female, when accounting for other predictors. Having a higher trip duration significantly increase odds, on average, of predicting a rider to be female, when accounting for other predictors. \n",
    "\n",
    "The nature of the relationship that we are trying to classify is inherently ambiguous, but these models do show that there is some potential in a model being able to classify a rider as male of female correctly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
