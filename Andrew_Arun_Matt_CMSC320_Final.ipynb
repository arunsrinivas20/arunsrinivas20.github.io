{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Citibike Data in NYC\n",
    "## Introduction\n",
    "\n",
    "The objective of this project is to analyze a large random sample of Citibike data from the years 2013 to 2019. We will look at different trends between location and month, as well as trends in the type of users and so much more. Hopefully after reading this tutorial, you will not only be able to understand more about different statistics/trends about the Citibike data, but also how much Citibike has become integrated in the lives of New Yorkers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python Modules\n",
    "The main python modules we will be using in our project are:\n",
    "1. folium\n",
    "2. requests\n",
    "3. pandas\n",
    "4. numpy\n",
    "5. re\n",
    "6. datetime\n",
    "7. BeautifulSoup\n",
    "8. json\n",
    "9. matplotlib\n",
    "10. sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium\n",
    "import requests #for get request\n",
    "import pandas as pd #pandas\n",
    "import numpy as np #module\n",
    "import re #regex\n",
    "from datetime import datetime #datetime objects\n",
    "from bs4 import BeautifulSoup #prettify's our content\n",
    "import json #needed for google API\n",
    "import os.path #needed for file reading\n",
    "import matplotlib.pyplot as plt #for plotting\n",
    "from sklearn import linear_model #for linear regression\n",
    "from sklearn.preprocessing import PolynomialFeatures #polynomial regression\n",
    "import folium\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "This is the data collection phase of the data life cycle. During this phase, our focus is on collecting and transforming the data into a usable form. In our case, that usable form is a pandas dataframe derived from a CSV file containing the original data. <br> <br>\n",
    "\n",
    "After doing a lot of research for a dataset, we started to look towards the travel industry for motivation. We were motivated by data that contained information regarding ride sharing. At the end, we finalized our search on publicly available bike sharing data from Citi Bike NYC. We retrieved the data from the following site: https://s3.amazonaws.com/tripdata/index.html?fbclid=IwAR3BJ9yWYcKtBRYUMQq2SI1IZR9AnFel3C-eTato4mWXtwBz4olhrdrai7Js\n",
    "\n",
    "The company had available several months worth of data, ranging from June of 2013 to October of 2019, so we decided to explore each of these months. In attempting to utilize the data from the entire data range mentioned, we needed to perform several steps to automate the download and collection process. The following process will describe how we went about collecting and manipulating the data into one aggregate dataframe containing a certain number of entries of each date. <br>\n",
    "\n",
    "To collect and store the data, we used the following libraries from above:\n",
    "1. urllib\n",
    "2. zipfile\n",
    "3. pandas\n",
    "\n",
    "As mentioned before, the data was organized by year, and then month, so it was quite predictable in terms of figuring out how to extract the data files from the site. However, there were some unique aspects that required some workarounds. We looped through each year and month pair from June of 2013 to October of 2019. All of the month numbers in the file names had 2 digits, so we need to make sure all months before October (10th month) were **prepended with a \"0\"**. Most of the zip files followed the format **\"{date}{month_2_digit}-citibike-tripdata.zip\"**. The remaining zip files had a slightly different name format of **\"{date}{month_2_digit}-citibike-tripdata.csv.zip\"**. To get work around this, we instituted a check in the loop to determine if the current year was after 2016. If so, then the url and zip file name would be updated accordingly. \n",
    "\n",
    "Once those alterations were completed, as necessary, we proceeded to download the file data. To do so, we utilized the library function **urllib.request.urlretrieve** to download the zip file, indicated by the corresponding url, to the corresponding file path location, which was the current directory in this case. With the zip file downloaded, we proceeded to extract the contents of the zip file, only one CSV file in this case, using zipfile **extractall** method and extracted it to the current directory. We also retrieved the name of the file using the zipfile **namelist** and indexing at 0, as there is always only one element (CSV file) in said list. <br>\n",
    "    \n",
    "With the CSV data file download and extracted from the corresponding zip file, we transferred it into a pandas dataframe object. We made sure to add year and moth column to differentiate these data points from entries in future CSV files. After that, we decided to add only a sample of the resulting dataframe to our aggregate dataframe. This was due to the fact there were too many entries in every CSV file, so it would be impractical and inefficient to collect every data from each CSV for exploratory data analysis. With that in mind, we decided to randomly sample 10000 rows from each dataframe to be used for data analysis using the pandas **sample** method without replacement. In sampling with n=10000, we get a good representation of the data for that specific date.<br>\n",
    "    \n",
    "One issue we ran into when generating each sampled dataframe was with the column (attribute) names. We realized that, after approximately 2017, the attribute names were capitalized. Due to this difference, the resulting aggregate dataframe at first contained many NaNs because pandas concatenates dataframes based on their column names. The dataframe resulted in having 2 times the normal amount of column names: half being lowercase and half being capitalized. Ignoring case, the column names across all CSV files were identical. To work around this, we decided to create a list of default, lowercase column names, **col_names**, and replaced the column names of each sampled dataframe with our list. <br>\n",
    "    \n",
    "After iterating over each date, we ended up with a **list of 77 dataframes**, each corresponding to a specific date from the data. Since, we made sure that all of these dataframes had the same column names, we proceeded to concatenate all of them into one aggregate dataframe, as mentioned before. With 77 dataframes, each with 10000 randomly sampled data points, the new aggregate dataframe contained **770001** total data points. To do this, we utilized the pandas **concat** function, because it we could pass an iterable, which, in this case, was our list of sampled dataframes. The resulting dataframe was returned as output for display purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The data begins starting on June of 2013. \n",
    "year = 2013\n",
    "month = 6\n",
    "\n",
    "# In some of the csv files, we found that the column names differed by \n",
    "# capitalization, so below is a standard list of the all of the column\n",
    "# names that we apply to each sampled dataframe. \n",
    "col_names = ['tripduration', 'starttime', 'stoptime', 'start station id',\n",
    " 'start station name', 'start station latitude', 'start station longitude',\n",
    " 'end station id', 'end station name', 'end station latitude',\n",
    " 'end station longitude', 'bikeid', 'usertype', 'birth year', 'gender', 'year',\n",
    " 'month']\n",
    "\n",
    "# A list to hold all of the dataframes\n",
    "list_dfs = []\n",
    "\n",
    "# The last csv file contains the data recorded on October of 2019. The\n",
    "# loop will increment by 1 month, until after this date is reached. \n",
    "while (not (year == 2019 and month == 11)):\n",
    "    date_str = f'{year}{month}'\n",
    "    \n",
    "    # If the month is before October (10), prepend the corresponding \n",
    "    # integer value with a 0 to make it 2 digits\n",
    "    if month < 10:\n",
    "        date_str = f'{year}0{month}'\n",
    "    \n",
    "    # Corresponding download URL for each relevant zip file\n",
    "    url = f'https://s3.amazonaws.com/tripdata/{date_str}-citibike-tripdata.zip'\n",
    "    # Corresponding name for each relevant zip file\n",
    "    zip_file = f'{date_str}-citibike-tripdata.zip'\n",
    "    \n",
    "    # Format of zip file name changes after 2016, so this if stmt accounts for that\n",
    "    if (year > 2016):\n",
    "        url = f'https://s3.amazonaws.com/tripdata/{date_str}-citibike-tripdata.csv.zip'\n",
    "        zip_file = f'{date_str}-citibike-tripdata.csv.zip'\n",
    "    \n",
    "    # Zip file path points to the current directory \".\"\n",
    "    zip_file_path = f'./{zip_file}'\n",
    "    \n",
    "    print(year, month)\n",
    "    \n",
    "    # Download the corresponding zip file\n",
    "    urllib.request.urlretrieve(url, zip_file)\n",
    "    \n",
    "    csv_filename = ''\n",
    "    \n",
    "    # Extract the csv from the zip file and get its name\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "        csv_filename = zip_ref.namelist()[0]\n",
    "    \n",
    "    # Read the csv file into a pandas dataframe\n",
    "    df = pd.read_csv(f'./{csv_filename}')\n",
    "    \n",
    "    # Add the year and month attributes to distinguish from entries\n",
    "    # from entries from other dates\n",
    "    df['year'] = year\n",
    "    df['month'] = month  \n",
    "    \n",
    "    # Take a random sample of 10000 data points from the csv\n",
    "    sampled_df = df.sample(10000, replace=False, random_state=1)  \n",
    "    \n",
    "    # Make sure that all of the dataframes have the same attribute formats\n",
    "    sampled_df.columns = list(col_names)    \n",
    "    print(sampled_df.columns.values)\n",
    "    \n",
    "    # Add the dataframe to a list containing all dataframes \n",
    "    list_dfs.append(sampled_df)   \n",
    "    \n",
    "    # Increment by 1 month. Increment the year by 1 if month goes past Dec.\n",
    "    month += 1  \n",
    "    if (month > 12):\n",
    "        month = 1\n",
    "        year += 1\n",
    "\n",
    "# Concatenate all dataframes together to form a cumulative dataframe \n",
    "bikes_df = pd.concat(list_dfs)     \n",
    "bikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the dataframe containing sampled observations from every date in the range of June of 2013 to October of 2019, we decided to export it to a CSV file for future use by using the pandas **to_csv** method. The reason behind this was because the process of downloading, extracting, and converting the data from the site was time and memory intensive. Another related thing to note was that kernel did die on occasion and had to restart. For this reason, rather than repeatedly performing these operations every time, we exported to the resulting dataframe to a CSV for efficiency purposes. It will later be converted back to a dataframe object when we begin our data analysis below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a csv file for later use and safe keeping\n",
    "bikes_df.to_csv('./FINAL_SAMPLED_DATA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the saved dataset (FINAL_SAMPLED_DATA.csv) from zip file here\n",
    "with zipfile.ZipFile('./FINAL_SAMPLED_DATA.csv.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('.')\n",
    "    csv_filename = zip_ref.namelist()[0]\n",
    "bikes_df = pd.read_csv(csv_filename)\n",
    "bikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we wanted to do is remove any incomplete data, especially those that had NaN for locations. For our heatmaps later on, we decided to construct them on the basis of start and end latitudes and longitudes. To do this, we utilizied the pandas dropna method to drop all rows that contained NaNs. We felt that this was safer than trying to impute values because, for example, it would be infeasible to replace unknown location values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikes_df = bikes_df.dropna()\n",
    "bikes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next turn the starttime and stoptime column values into datetime objects. This will help us later on in the analysis when we need to access individual elements of the time (hour, day, year etc.). The format of the datetime object will be 'year-month-day hour:minute:second:millisecond'. We will be using Python's build-in function 'to_datetime' to help us convert the strings into datetime objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert starttime column values into datetime objects\n",
    "bikes_df['starttime'] = pd.to_datetime(bikes_df['starttime'])\n",
    "\n",
    "# Convert stoptime column values into datetime objects\n",
    "bikes_df['stoptime'] = pd.to_datetime(bikes_df['stoptime'])\n",
    "\n",
    "# Print the resulting dataframe\n",
    "bikes_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis and Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be plotting our data and analyzing trends that may be significant. Some plots you will see are most popular starting points of trips, comparison between subscriber and customer of Citibikes, the day that most people use the Citibikes, and so much more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Frequency of Start Time of Trip\n",
    "We will be plotting the number of occurrences during the day when people begin their trip. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_of_day = {}\n",
    "\n",
    "# Fill time_of_day hash table with 0 occurrences of users for each hour\n",
    "for i in range(0, 24):\n",
    "    time_of_day[i] = 0\n",
    "\n",
    "# Loop through dataframe and increase counts of users by one \n",
    "# for that hour if the user started the trip at that hour\n",
    "for index, row in bikes_df.iterrows():\n",
    "    datetime_obj = row['starttime']\n",
    "    time_of_day[datetime_obj.hour] += 1\n",
    "\n",
    "# We will now plot histogram of the time of day people start using the bikes.\n",
    "plt.bar(time_of_day.keys(), time_of_day.values(), align='center')\n",
    "plt.xlabel('Hour in Day (0-23)')\n",
    "plt.ylabel('Number of Users Starting Trip')\n",
    "plt.title('Hour in Day vs. Number of Users Starting Trip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the graph above, most of the people rent Citibikes around 8am and 5pm. The reason for these two peaks at these two times could be that these two times correspond to when most people are going to work and leaving work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Most Popular Days of the Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "week_day= {}\n",
    "\n",
    "# Fill time_of_day hash table with 0 occurrences of users for each hour\n",
    "for i in range(0, 7):\n",
    "    week_day[i] = 0\n",
    "\n",
    "# Loop through dataframe and increase counts of users by one \n",
    "# for that hour if the user started the trip at that hour\n",
    "for index, row in bikes_df.iterrows():\n",
    "    datetime_obj = row['starttime']\n",
    "    week_day[datetime_obj.weekday()] += 1\n",
    "\n",
    "# We will now plot histogram of the time of day people start using the bikes.\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plt.bar(week_day.keys(), week_day.values(), align='center')\n",
    "plt.xticks(list(week_day.keys()), days, rotation=35)\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Number of Users Starting Trip')\n",
    "plt.title('Day of the Week vs. Number of Users Starting Trip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding and Analyzing Source/Destination Stations\n",
    "#### Heatmaps of Source/Destination Stations\n",
    "We will be plotting the heatmap of source and destination locations. By plotting this, we will hopefully be able to analyze locations where most users retrieve and dock their bikes at in NYC. The brighter an area is (more orange/yellow) the more users that were in that area retrieving/docking their bikes. Feel free to zoom in too to take a closer look at the different areas in the city where most people are using Citibikes. \n",
    "\n",
    "The following is the Source Stations Heatmap: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below will plot the heatmap of source locations \n",
    "from folium.plugins import HeatMap\n",
    "map_osm = folium.Map(location=[40.7358, -73.9760], zoom_start=11.5)\n",
    "\n",
    "# Create HeatMap of the crimes in Baltimore\n",
    "bikes_df['count'] = 1\n",
    "HeatMap(data=bikes_df[['start station latitude', 'start station longitude', 'count']]\n",
    "        .groupby(['start station latitude', 'start station longitude'])\n",
    "        .sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(map_osm)\n",
    "\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the Destination Stations HeatMap: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below will plot the heatmap of destination locations \n",
    "map_osm = folium.Map(location=[40.7358, -73.9760], zoom_start=11.5)\n",
    "\n",
    "# Create HeatMap of the crimes in Baltimore\n",
    "bikes_df['count'] = 1\n",
    "HeatMap(data=bikes_df[['end station latitude', 'end station longitude', 'count']]\n",
    "        .groupby(['end station latitude', 'end station longitude'])\n",
    "        .sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(map_osm)\n",
    "\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the two maps above, at first glance, we can see that most people start their trips in the heart of Manhattan and Brooklyn. If we zoom in more, we can get a greater sense of where most people start/end their trips from. In the source destination heatmap, we can that around 45th street and the southernest part of Manhattan, there is a good amount of trips that start from the stations near there. Likewise, we can see the same trend in the destination heatmap. We can next plot the most popular starting stations and see them on the map.  \n",
    "\n",
    "#### Plotting Popular Source/Destination Stations\n",
    "Instead of plotting a heatmap, we can plot the top 10 source stations as well as the top 10 destination stations from all the years 2013-2019. By plotting this, we can see more about where these stations are on the map and see if they correspond to the high concentration of users on the heatmap above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stations_start = {}\n",
    "stations_end = {}\n",
    "\n",
    "# Loop through dataframe and keep count of the number of people who leave \n",
    "# from the stations and arrive at the stations\n",
    "for index, row in bikes_df.iterrows():\n",
    "    station_name_start = row['start station name']\n",
    "    station_name_end = row['end station name']\n",
    "    \n",
    "    if station_name_start not in stations_start:\n",
    "        stations_start[station_name_start] = 0\n",
    "        \n",
    "    if station_name_end not in stations_end:\n",
    "        stations_end[station_name_end] = 0\n",
    "        \n",
    "    stations_start[station_name_start] += 1\n",
    "    stations_end[station_name_end] += 1\n",
    "\n",
    "# Sort the hash by value and retrieve top 10 source/destination stations\n",
    "stations_start = sorted(stations_start.items(), key=lambda x:-x[1])[:10]\n",
    "stations_end = sorted(stations_end.items(), key=lambda x:-x[1])[:10]\n",
    "\n",
    "# Extract first 10 highest items from hash (stations with most users)\n",
    "# and fill up the x and y lists that we will plot\n",
    "top_x_start = []\n",
    "top_y_start = []\n",
    "top_x_end = []\n",
    "top_y_end = []\n",
    "for key, value in stations_start:\n",
    "    top_x_start.append(key)\n",
    "    top_y_start.append(value)\n",
    "    \n",
    "for key, value in stations_end:\n",
    "    top_x_end.append(key)\n",
    "    top_y_end.append(value)\n",
    "\n",
    "# We will now plot histogram of the top 10 stations that people start and end\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.bar(top_x_start, top_y_start, align='center')\n",
    "ax1.set_xlabel('Station Names')\n",
    "ax1.set_xticklabels(top_x_start, rotation = 45, ha='right')\n",
    "ax1.set_ylabel('Number of Users Starting Trip at that Station')\n",
    "ax1.set_title('Top 10 Source Stations with Most Users')\n",
    "\n",
    "fig, ax2 = plt.subplots()\n",
    "ax2.bar(top_x_end, top_y_end, align='center')\n",
    "ax2.set_xlabel('Station Names')\n",
    "ax2.set_xticklabels(top_x_end, rotation = 45, ha='right')\n",
    "ax2.set_ylabel('Number of Users Ending Trip at that Station')\n",
    "ax2.set_title('Top 10 Destination Stations with Most Users')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see what the most popular stations are in the histogram, but now let's see where they are on the map. First, we will graph the most popular source stations below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_osm = folium.Map(location=[40.7300, -74.0007], zoom_start=13)\n",
    "\n",
    "# Add points to map to show most popular source stations\n",
    "for name in top_x_start:\n",
    "    # Retrieve the latitude and longitude of that location in dataframe     \n",
    "    row = bikes_df.loc[bikes_df['start station name'] == name].iloc[0]\n",
    "    lat = row['start station latitude']\n",
    "    long = row['start station longitude']\n",
    "    \n",
    "    # Add a circle indicating the location in NYC\n",
    "    folium.Circle(\n",
    "        radius=20,\n",
    "        location=[lat, long],\n",
    "        popup=name,\n",
    "        color='black',\n",
    "        fill=True,\n",
    "    ).add_to(map_osm)\n",
    "\n",
    "map_osm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we will graph the top 10 destination stations below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_osm = folium.Map(location=[40.7300, -74.0007], zoom_start=13)\n",
    "\n",
    "# Add points to map to show most popular destination stations\n",
    "for name in top_x_end:\n",
    "    # Retrieve the latitude and longitude of that location in dataframe     \n",
    "    row = bikes_df.loc[bikes_df['end station name'] == name].iloc[0]\n",
    "    lat = row['end station latitude']\n",
    "    long = row['end station longitude']\n",
    "    \n",
    "    # Add a circle indicating the location in NYC      \n",
    "    folium.Circle(\n",
    "        radius=20,\n",
    "        location=[lat, long],\n",
    "        popup=name,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "    ).add_to(map_osm)\n",
    "\n",
    "map_osm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: We write the conclusion here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Habits Between Subscribers and Customers (day-pass and single-ride users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = bikes_df.loc[bikes_df['usertype'] == \"Customer\"]\n",
    "subscribers = bikes_df.loc[bikes_df['usertype'] == \"Subscriber\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer vs Subscriber Frequency By Day of Week\n",
    "\n",
    "TODO: Standardize for better comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare day of week,\n",
    "week_day_sub= {}\n",
    "week_day_cust= {}\n",
    "\n",
    "# Fill time_of_day hash table with 0 occurrences of users for each hour\n",
    "for i in range(0, 7):\n",
    "    week_day_sub[i] = 0\n",
    "    week_day_cust[i] = 0\n",
    "\n",
    "# Loop through dataframe and increase counts of users by one \n",
    "# for that hour if the user started the trip at that hour\n",
    "for index, row in bikes_df.iterrows():\n",
    "    datetime_obj = row['starttime']\n",
    "    if row['usertype'] == 'Customer':\n",
    "        week_day_cust[datetime_obj.weekday()] += 1\n",
    "    elif row['usertype'] == 'Subscriber':\n",
    "        week_day_sub[datetime_obj.weekday()] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar( week_day_sub.keys(), week_day_sub.values())\n",
    "plt.bar( week_day_cust.keys(), week_day_cust.values())\n",
    "\n",
    "days = ['Monday','Tuesday','Wednesday','Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.xticks(list(week_day_cust.keys()), days, rotation=35)\n",
    "plt.ylabel('Number of Users Starting Trip')\n",
    "plt.title('Day of the Week vs. Number of Users Starting Trip')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer vs Subscriber Frequency By Time of Day\n",
    "\n",
    "TODO: Standardize for better comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare time of day\n",
    "\n",
    "# compare day of week,\n",
    "time_sub= {}\n",
    "time_cust= {}\n",
    "\n",
    "# Fill time_of_day hash table with 0 occurrences of users for each hour\n",
    "for i in range(0, 24):\n",
    "    time_sub[i] = 0\n",
    "    time_cust[i] = 0\n",
    "\n",
    "# Loop through dataframe and increase counts of users by one \n",
    "# for that hour if the user started the trip at that hour\n",
    "for index, row in bikes_df.iterrows():\n",
    "    datetime_obj = row['starttime']\n",
    "    if row['usertype'] == 'Customer':\n",
    "        time_cust[datetime_obj.hour] += 1\n",
    "    elif row['usertype'] == 'Subscriber':\n",
    "        time_sub[datetime_obj.hour] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(list(time_sub.keys()), list(time_sub.values()))\n",
    "plt.bar(list(time_cust.keys()), list(time_cust.values()))\n",
    "plt.xlabel('Hour in Day (0-23)')\n",
    "plt.ylabel('Number of Users Starting Trip')\n",
    "plt.title('Hour in Day vs. Number of Users Starting Trip')\n",
    "plt.show()\n",
    "\n",
    "# compare locations\n",
    "# popular tourist locations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap of Trips for Customers vs Subscribers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "map_osm = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "\n",
    "bikes_df['count'] = 1\n",
    "HeatMap(data=customers[['end station latitude', 'end station longitude']]\n",
    "        .groupby(['end station latitude', 'end station longitude'])\n",
    "        .sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(map_osm)\n",
    "\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_osm = folium.Map(location=[40.7128, -74.0060], zoom_start=11)\n",
    "\n",
    "bikes_df['count'] = 1\n",
    "HeatMap(data=subscribers[['end station latitude', 'end station longitude']]\n",
    "        .groupby(['end station latitude', 'end station longitude'])\n",
    "        .sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(map_osm)\n",
    "\n",
    "map_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis, Hypothesis Testing and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insight and Policy Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
